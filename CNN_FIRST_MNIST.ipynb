{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('data/MNIST/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolution layer1 parameters\n",
    "f_size1 = 5\n",
    "num_f1= 16\n",
    "\n",
    "# Convolution layer1 parameters\n",
    "f_size2 = 5\n",
    "num_f2= 36\n",
    "\n",
    "#number of hidden units/neurons in fully connected layer\n",
    "fc_size = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set : 55000\n",
      "Validation set : 5000\n",
      "Test set : 10000\n"
     ]
    }
   ],
   "source": [
    "# Different train/validation/test size in dataset\n",
    "print(\"Training set : {}\".format(len(data.train.labels)))\n",
    "print(\"Validation set : {}\".format(len(data.validation.labels)))\n",
    "print(\"Test set : {}\".format(len(data.test.labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating test labels with correct classes\n",
    "data.test.cls= np.argmax(data.test.labels,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data dimensions\n",
    "#Image size \n",
    "img_size = 28\n",
    "\n",
    "#Image flatten size, MNIST dataset images contains only single channel images\n",
    "img_flat = img_size* img_size\n",
    "\n",
    "#Single channel gray scale images\n",
    "num_channel= 1\n",
    "\n",
    "#number of classed\n",
    "num_class= 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images,class_true,class_pred=None):\n",
    "    \n",
    "    assert len(images) == len(class_true) == 9\n",
    "    \n",
    "    fig, axes= plt.subplots(3,3)\n",
    "    fig.subplots_adjust(hspace=.5, wspace=.5)\n",
    "    \n",
    "    for i , ax in enumerate(axes.flat):\n",
    "        \n",
    "        ax.imshow(images[i].reshape(img_size,img_size),cmap=\"binary\")\n",
    "        \n",
    "        if class_pred is None:\n",
    "            tlabel= \"True: {}\".format(class_true[i])\n",
    "        else:\n",
    "            tlabel=\"True: {} Pred: {}\".format(class_true[i],class_pred[i])\n",
    "        #Set label on x axis of image    \n",
    "        ax.set_xlabel(tlabel)\n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUYAAAD5CAYAAABbPJzkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26e04b01400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Ploting first 9 imgaes of numbers to test plot_images function\n",
    "plot_images(data.test.images[0:9],data.test.cls[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for creating new convolution layer\n",
    "def new_convolution_layer(input_data,\n",
    "                         num_input_channel,\n",
    "                         filter_size,\n",
    "                         num_filter,\n",
    "                         max_pool=True):\n",
    "    #shape of filter weights for convolution\n",
    "    sh=[filter_size,filter_size,num_input_channel,num_filter]\n",
    "    #weight is a parameter which has to be updated internally during optimization process \n",
    "    #which means weights has to be a variable in tensorflow\n",
    "    \n",
    "    W= tf.Variable(tf.truncated_normal(sh, stddev=0.05))\n",
    "    b= tf.Variable(tf.constant(0.05, shape=[num_filter]))\n",
    "    \n",
    "    layer =  tf.nn.conv2d(input=input_data,\n",
    "                         filter=W ,\n",
    "                         strides=[1,1,1,1],\n",
    "                         padding='SAME')\n",
    "    layer +=b\n",
    "    if max_pool:\n",
    "        layer=tf.nn.max_pool(value=layer,\n",
    "                      ksize=[1, 2, 2, 1],\n",
    "                       strides=[1, 2, 2, 1],\n",
    "                       padding='SAME')\n",
    "        \n",
    "    #relu rectifier activation function zero all the negative pixels and add some non linearity to the function which \n",
    "    # enables the network to learn more complicated functions\n",
    "    layer=tf.nn.relu(layer)\n",
    "    \n",
    "    return layer, W\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Flattening a layer is new layer item will be imghight*imgweight*channels\n",
    "def flatten_layer(layer):\n",
    "     # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function for creating fully connected layer\n",
    "#it will have two parameters Weights and baise\n",
    "#Weights size will be (number of input from previous layer, number of neurons in current layer)\n",
    "#Bais size will be (numof neurons in current layer,1)\n",
    "def new_fc_layer(layer_flat,\n",
    "                 input_size,\n",
    "                 output_size,\n",
    "                 use_relu = True):\n",
    "    W=tf.Variable(tf.truncated_normal([input_size,output_size], stddev=0.05))\n",
    "    b= tf.Variable(tf.constant(0.05, shape=[output_size]))\n",
    "    \n",
    "    layer = tf.matmul(layer_flat, W) + b\n",
    "    \n",
    "    if use_relu:\n",
    "        layer= tf.nn.relu(layer)\n",
    "    \n",
    "    return layer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place holders for passing values to computation graphs\n",
    "x= tf.placeholder(tf.float32,shape=[None,img_flat],name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image=tf.reshape(x,[-1,img_size,img_size,num_channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y= tf.placeholder(tf.float32,shape=[None,num_class],name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_image=tf.argmax(y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates first convolution layer\n",
    "layer_1,W_1 = new_convolution_layer(x_image,num_channel,f_size1,num_f1,max_pool=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu:0' shape=(?, 14, 14, 16) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(5, 5, 1, 16) dtype=float32_ref>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates second convolution layer\n",
    "layer_2,w_2=new_convolution_layer(layer_1,num_f1,f_size2,num_f2,max_pool=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_1:0' shape=(?, 7, 7, 36) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_2:0' shape=(5, 5, 16, 36) dtype=float32_ref>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2_flat,num_feature=flatten_layer(layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_1:0' shape=(?, 1764) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer2_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_3 = new_fc_layer(layer2_flat,num_feature,fc_size,use_relu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_2:0' shape=(?, 128) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_4 = new_fc_layer(layer_3,fc_size,num_class,use_relu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_3:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The second fully-connected layer estimates how likely it is that the input image belongs to each of the 10 classes. \n",
    "#However, these estimates are a bit rough and difficult to interpret because the numbers may be very small or large, \n",
    "#so we want to normalize them so that each element is limited between zero and one and the 10 elements sum to one. \n",
    "#This is calculated using the so-called softmax function and the result is stored in y_pred.\n",
    "y_pred=tf.nn.softmax(layer_4)\n",
    "y_pred_class= tf.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-30-b7fb078d1fb8>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_4,\n",
    "                                                        labels=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimization\n",
    "optimizer= tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred_class, y_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "session= tf.Session()\n",
    "#we need to initailize the variables W,b first before starting optimization\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mini batches having size 64,128,256 etc\n",
    "train_batch_size=64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counter for total number of iterations performed so far.\n",
    "total_iterations = 0\n",
    "\n",
    "def optimize(num_iterations):\n",
    "    # Ensure we update the global variable rather than a local copy.\n",
    "    global total_iterations\n",
    "\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "        x_batch, y_true_batch = data.train.next_batch(train_batch_size)\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch,\n",
    "                           y: y_true_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "        # Print status every 100 iterations.\n",
    "        if i % 100 == 0:\n",
    "            # Calculate the accuracy on the training-set.\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "            # Message for printing.\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "\n",
    "            # Print it.\n",
    "            print(msg.format(i + 1, acc))\n",
    "\n",
    "    # Update the total number of iterations performed.\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_batch_size = 256\n",
    "\n",
    "def print_test_accuracy(show_example_errors=False,\n",
    "                        show_confusion_matrix=False):\n",
    "\n",
    "    # Number of images in the test-set.\n",
    "    num_test = len(data.test.images)\n",
    "\n",
    "    # Allocate an array for the predicted classes which\n",
    "    # will be calculated in batches and filled into this array.\n",
    "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "\n",
    "    # Now calculate the predicted classes for the batches.\n",
    "    # We will just iterate through all the batches.\n",
    "    # There might be a more clever and Pythonic way of doing this.\n",
    "\n",
    "    # The starting index for the next batch is denoted i.\n",
    "    i = 0\n",
    "\n",
    "    while i < num_test:\n",
    "        # The ending index for the next batch is denoted j.\n",
    "        j = min(i + test_batch_size, num_test)\n",
    "\n",
    "        # Get the images from the test-set between index i and j.\n",
    "        images = data.test.images[i:j, :]\n",
    "\n",
    "        # Get the associated labels.\n",
    "        labels = data.test.labels[i:j, :]\n",
    "\n",
    "        # Create a feed-dict with these images and labels.\n",
    "        feed_dict = {x: images,\n",
    "                     y: labels}\n",
    "\n",
    "        # Calculate the predicted class using TensorFlow.\n",
    "        cls_pred[i:j] = session.run(y_pred_class, feed_dict=feed_dict)\n",
    "\n",
    "        # Set the start-index for the next batch to the\n",
    "        # end-index of the current batch.\n",
    "        i = j\n",
    "\n",
    "    # Convenience variable for the true class-numbers of the test-set.\n",
    "    cls_true = data.test.cls\n",
    "\n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    # Calculate the number of correctly classified images.\n",
    "    # When summing a boolean array, False means 0 and True means 1.\n",
    "    correct_sum = correct.sum()\n",
    "\n",
    "    # Classification accuracy is the number of correctly classified\n",
    "    # images divided by the total number of images in the test-set.\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(acc, correct_sum, num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test-Set: 7.1% (711 / 10000)\n"
     ]
    }
   ],
   "source": [
    "print_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:      1, Training Accuracy:   3.1%\n",
      "Time usage: 0:00:01\n"
     ]
    }
   ],
   "source": [
    "optimize(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:    101, Training Accuracy:  64.1%\n",
      "Time usage: 0:00:14\n"
     ]
    }
   ],
   "source": [
    "optimize(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:    201, Training Accuracy:  92.2%\n",
      "Optimization Iteration:    301, Training Accuracy:  89.1%\n",
      "Optimization Iteration:    401, Training Accuracy:  92.2%\n",
      "Optimization Iteration:    501, Training Accuracy:  87.5%\n",
      "Optimization Iteration:    601, Training Accuracy:  82.8%\n",
      "Optimization Iteration:    701, Training Accuracy:  90.6%\n",
      "Optimization Iteration:    801, Training Accuracy:  92.2%\n",
      "Optimization Iteration:    901, Training Accuracy:  93.8%\n",
      "Optimization Iteration:   1001, Training Accuracy:  92.2%\n",
      "Time usage: 0:02:43\n"
     ]
    }
   ],
   "source": [
    "optimize(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test-Set: 93.3% (9335 / 10000)\n"
     ]
    }
   ],
   "source": [
    "print_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
